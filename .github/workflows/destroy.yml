name: Terraform - Destroy Infrastructure

on:
  workflow_dispatch:
    inputs:
      confirm_destroy:
        description: 'Type "DESTROY" to confirm infrastructure destruction'
        required: true
        type: string

env:
  AWS_REGION: ${{ secrets.AWS_REGION }}
  EKS_CLUSTER_NAME: abalone-mlops

jobs:
  destroy-infrastructure:
    name: Destroy AWS Infrastructure
    runs-on: ubuntu-latest
    
    steps:
    - name: Validate Destroy Confirmation
      run: |
        if [ "${{ github.event.inputs.confirm_destroy }}" != "DESTROY" ]; then
          echo "‚ùå Destroy confirmation failed. You must type 'DESTROY' to proceed."
          exit 1
        fi
        echo "‚úÖ Destroy confirmation validated"

    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.TERRAFORM_AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.TERRAFORM_AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.5.0
        terraform_wrapper: false

    - name: Configure kubectl for EKS (if exists)
      run: |
        echo "üîç Checking if EKS cluster exists..."
        if aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
          echo "‚úÖ EKS cluster found, configuring kubectl..."
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
          kubectl cluster-info
          echo "EKS_EXISTS=true" >> $GITHUB_ENV
        else
          echo "‚ÑπÔ∏è EKS cluster not found or already destroyed"
          echo "EKS_EXISTS=false" >> $GITHUB_ENV
        fi

    - name: Clean Up All Kubernetes Resources
      if: env.EKS_EXISTS == 'true'
      run: |
        echo "üßπ Cleaning up all Kubernetes resources that might have LoadBalancers..."
        
        # Delete all LoadBalancer services across all namespaces
        echo "üîç Finding and deleting LoadBalancer services..."
        kubectl get svc --all-namespaces -o json | jq -r '.items[] | select(.spec.type=="LoadBalancer") | "\(.metadata.namespace) \(.metadata.name)"' | while read namespace service; do
          if [ -n "$namespace" ] && [ -n "$service" ]; then
            echo "üóëÔ∏è Deleting LoadBalancer service: $service in namespace: $namespace"
            kubectl delete svc "$service" -n "$namespace" --ignore-not-found=true --timeout=300s
          fi
        done
        
        # Delete common application namespaces
        echo "üóëÔ∏è Deleting application namespaces..."
        for ns in mlflow default kube-system; do
          if [ "$ns" != "kube-system" ]; then
            echo "Deleting namespace: $ns"
            kubectl delete namespace "$ns" --ignore-not-found=true --timeout=300s &
          fi
        done
        
        # Wait for namespace deletions
        wait
        
        # Force delete any remaining LoadBalancer services
        echo "üîç Double-checking for remaining LoadBalancer services..."
        kubectl get svc --all-namespaces --field-selector spec.type=LoadBalancer --no-headers 2>/dev/null | while read line; do
          if [ -n "$line" ]; then
            namespace=$(echo $line | awk '{print $1}')
            service=$(echo $line | awk '{print $2}')
            echo "üóëÔ∏è Force deleting remaining LoadBalancer: $service in $namespace"
            kubectl delete svc "$service" -n "$namespace" --force --grace-period=0 --ignore-not-found=true
          fi
        done
        
        echo "‚è≥ Waiting for LoadBalancers to be fully deleted..."
        sleep 120

    - name: Clean Up AWS Load Balancers Manually
      run: |
        echo "üîç Checking for remaining AWS Load Balancers..."
        
        # Clean up Application Load Balancers
        echo "üóëÔ∏è Cleaning up Application Load Balancers..."
        aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s`) || contains(LoadBalancerName, `abalone`)].LoadBalancerArn' --output text | while read lb_arn; do
          if [ -n "$lb_arn" ] && [ "$lb_arn" != "None" ]; then
            echo "Deleting ALB: $lb_arn"
            aws elbv2 delete-load-balancer --load-balancer-arn "$lb_arn" || true
          fi
        done
        
        # Clean up Classic Load Balancers
        echo "üóëÔ∏è Cleaning up Classic Load Balancers..."
        aws elb describe-load-balancers --query 'LoadBalancerDescriptions[?contains(LoadBalancerName, `k8s`) || contains(LoadBalancerName, `abalone`)].LoadBalancerName' --output text | while read lb_name; do
          if [ -n "$lb_name" ] && [ "$lb_name" != "None" ]; then
            echo "Deleting ELB: $lb_name"
            aws elb delete-load-balancer --load-balancer-name "$lb_name" || true
          fi
        done
        
        echo "‚è≥ Waiting for Load Balancers to be fully deleted..."
        sleep 60

    - name: Clean Up Security Groups
      run: |
        echo "üîç Cleaning up security groups with dependencies..."
        
        # Get VPC ID from terraform state if available
        VPC_ID=""
        if [ -f "infrastructure/terraform.tfstate" ]; then
          VPC_ID=$(grep -o '"vpc_id":"vpc-[^"]*"' infrastructure/terraform.tfstate | cut -d'"' -f4 | head -1)
        fi
        
        if [ -z "$VPC_ID" ]; then
          # Try to find VPC by tags
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=abalone-mlops-vpc" --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "")
        fi
        
        if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
          echo "üîç Found VPC: $VPC_ID"
          
          # Delete security groups (except default)
          aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text | while read sg_id; do
            if [ -n "$sg_id" ] && [ "$sg_id" != "None" ]; then
              echo "üóëÔ∏è Attempting to delete security group: $sg_id"
              aws ec2 delete-security-group --group-id "$sg_id" 2>/dev/null || echo "Could not delete $sg_id (may have dependencies)"
            fi
          done
        fi

    - name: Clean Up Network Interfaces
      run: |
        echo "üîç Cleaning up network interfaces..."
        
        # Delete available network interfaces that might be blocking subnet deletion
        aws ec2 describe-network-interfaces --filters "Name=status,Values=available" --query 'NetworkInterfaces[].NetworkInterfaceId' --output text | while read eni_id; do
          if [ -n "$eni_id" ] && [ "$eni_id" != "None" ]; then
            echo "üóëÔ∏è Deleting network interface: $eni_id"
            aws ec2 delete-network-interface --network-interface-id "$eni_id" 2>/dev/null || echo "Could not delete $eni_id"
          fi
        done

    - name: Initialize Terraform
      working-directory: ./infrastructure
      run: |
        echo "üîß Initializing Terraform..."
        terraform init \
          -backend-config="bucket=${{ secrets.S3_BUCKET_NAME }}" \
          -backend-config="key=terraform.tfstate" \
          -backend-config="region=${{ secrets.AWS_REGION }}"

    - name: Terraform Destroy with Retry
      working-directory: ./infrastructure
      run: |
        echo "üóëÔ∏è Starting Terraform destroy with retry logic..."
        
        # Function to run terraform destroy with retries
        destroy_with_retry() {
          local max_attempts=3
          local attempt=1
          
          while [ $attempt -le $max_attempts ]; do
            echo "üîÑ Terraform destroy attempt $attempt/$max_attempts..."
            
            if terraform destroy -auto-approve \
              -var="aws_region=${{ secrets.AWS_REGION }}" \
              -var="github_repo=${{ github.repository }}" \
              -target=module.eks \
              -target=aws_db_instance.mlflow_db \
              -target=aws_lambda_function.trigger_deployment; then
              echo "‚úÖ Targeted resources destroyed successfully"
              break
            else
              echo "‚ùå Targeted destroy attempt $attempt failed"
              if [ $attempt -eq $max_attempts ]; then
                echo "‚ö†Ô∏è Continuing with full destroy after targeted failures..."
              else
                echo "‚è≥ Waiting 60 seconds before retry..."
                sleep 60
              fi
            fi
            
            attempt=$((attempt + 1))
          done
          
          # Full destroy
          echo "üîÑ Running full terraform destroy..."
          attempt=1
          while [ $attempt -le $max_attempts ]; do
            echo "üóëÔ∏è Full destroy attempt $attempt/$max_attempts..."
            
            if terraform destroy -auto-approve \
              -var="aws_region=${{ secrets.AWS_REGION }}" \
              -var="github_repo=${{ github.repository }}"; then
              echo "‚úÖ Infrastructure destroyed successfully!"
              return 0
            else
              echo "‚ùå Destroy attempt $attempt failed"
              if [ $attempt -lt $max_attempts ]; then
                echo "‚è≥ Waiting 120 seconds before retry..."
                sleep 120
              fi
            fi
            
            attempt=$((attempt + 1))
          done
          
          echo "‚ùå All destroy attempts failed"
          return 1
        }
        
        # Run destroy with retry
        destroy_with_retry

    - name: Manual Cleanup of Remaining Resources
      if: failure()
      run: |
        echo "üßπ Manual cleanup of remaining resources..."
        
        # Clean up any remaining EKS resources
        if aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
          echo "üóëÔ∏è Manually deleting EKS cluster..."
          aws eks delete-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} || true
        fi
        
        # Clean up RDS instances
        echo "üóëÔ∏è Cleaning up RDS instances..."
        aws rds describe-db-instances --query 'DBInstances[?contains(DBInstanceIdentifier, `mlflow`)].DBInstanceIdentifier' --output text | while read db_id; do
          if [ -n "$db_id" ] && [ "$db_id" != "None" ]; then
            echo "Deleting RDS instance: $db_id"
            aws rds delete-db-instance --db-instance-identifier "$db_id" --skip-final-snapshot --delete-automated-backups || true
          fi
        done
        
        echo "‚ö†Ô∏è Some resources may need manual cleanup in AWS Console"

    - name: Final Status
      run: |
        echo ""
        echo "üéØ DESTROY COMPLETED"
        echo "==================="
        echo ""
        echo "‚úÖ Infrastructure destroy process completed"
        echo ""
        echo "üîç If any resources remain:"
        echo "1. Check AWS Console for remaining resources"
        echo "2. Look for LoadBalancers, Security Groups, ENIs"
        echo "3. Delete them manually if needed"
        echo "4. Re-run this workflow if necessary"
        echo ""
        echo "üí° Remember to also clean up:"
        echo "- S3 bucket: ${{ secrets.S3_BUCKET_NAME }}"
        echo "- ECR repositories (if not needed)" 